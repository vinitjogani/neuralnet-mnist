# Neural Network classifier for MNIST dataset
Due to GitHub upload restrictions, the train and test datasets are not on the repository and must be download seperately from [here](https://pjreddie.com/projects/mnist-in-csv/). This classifier is simple and yet achieves an accuracy of approximately 92.1%. You can significantly improve this by running it longer (more epochs get closer, larger batch size to reduce error), and on a GPU, or by operating on uncompressed features - however that is simple not feasible on a normal CPU.

The first idea used here is dimensionality reduction using Principal Component Analysis which is used to compress the input images from 784 pixels to 500 pixels preserving 95% variance. This involves finding the covariance matrix of the feature vectors, finding its eigenvectors and then selecting the first 500 of them. This speeds up the neural network training process significantly when run on 60,000 training examples.

The data is then fed to a standard neural network with one hidden layer and 450 hidden units, which is trained using the backpropogation algorithm using mini-batch gradient descent. The batch size used here was 60, and the entire dataset was traversed (number of epochs = ) 3 times. Also note that instead of using the more common cross entropy loss function with softmax, I simply tried to minimize the mean squared error for efficiency reasons.
